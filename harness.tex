% Subsection 3.3: harness

\subsection{DPS: Distributed Processing Services}
\label{DPS}\label{sMw-har}

The Pipeline Harness is encapsulated within the Distributed Processing Services (DPS) package
and uses the namespace \code{lsst::dps}.  
The distributed processing framework of the \code{dps} package provides the ability to run an application 
pipeline in parallel across numerous nodes of a cluster or supercomputer, or
across cores of a workstation, 
in transparent fashion, shielding the application codes and their developers from the details
of parallel processing.  It provides the mechanism for integrating a piece of application code 
into  a pipeline as a single stage, and provides the infrastructure for combining multiple 
application stages in sequence to form a full pipeline. It also facilitates the  triggering of 
processing within pipelines through its use of the events system of the \code{events} package
(\Sec{sMw-ev}).  
In DC2,  \code{dps}
package classes were implemented in Python and C++, and the Message Passing Interface
(MPI) was used for parallel processing and communication. 

% A wide range of the pipeline functionality within the DPS package is configureable through  policy.

\subsubsection{Pipeline and Slice}

\paragraph{Description and Usage}

The LSST baseline model calls for classes \code{Pipeline} and \code{Slice} to encapsulate the serial and parallel 
threads of pipeline processing, respectively.  For DC2, \code{Pipeline} and \code{Slice} classes were implemented
in a mixture of Python and C++.  Most of the policy configuration, stage management,  and processing workflow 
and triggering was encoded within the Python classes, while the C++ classes strictly managed 
the MPI environment.  For DC2,  MPICH2 v1.0.5\footnote{http://www.mcs.anl.gov/research/projects/mpich2}
was the message passing library utilized.  
Furthermore, serial Pipelines made use of explicit MPI2 spawn functionality to launch $N$ \code{Slice} 
workers across a collection of nodes.   The MPI communication between \code{Pipeline} and \code{Slice}s was 
collective in nature and intercommunicator-based (i.e., accomplished using MPI\_Bcast 
and MPI\_Barrier calls across the intercommunicator generated by the MPI2 spawn). 
MPI communication of complex types such as \code{DataProperty}'s between \code{Pipeline} and \code{Slice}s 
was left out of the scope of DC2, and was instead accomplished via the \code{events} system for 
this version of the harness. 

Within DC2, \code{Pipeline} and \code{Slice} were configurable through \code{Policy}, and each of the parallel threads 
read the pipeline policy file to obtain required information for setup (policy parameters are 
not communicated through MPI).  Values typically included in a pipeline policy file include the 
names of the application stages to be executed  by the pipeline, the event triggering dependencies 
of the stages, the events system broker to be used, an event topic for clean system shutdown,
and others. 

\paragraph{Issues and Lessons Learned}

There are several issues with respect to current MPI usage that are worth consideration.
The use of collective MPI communications offered a fairly clean and simple approach for DC2, 
but introduced some fault tolerance issues, as a single \code{Slice} failing could impact the full 
complement of \code{Slice}s adversely.   Some extra coding was required to keep the broader 
collection of \code{Slice}s  executing in the event that one \code{Slice} 
encountered an exception, and it  is not clear that this is the cleanest solution.
Beyond the matter of collective communication, we also need to consider whether a single \code{Pipeline}
communicating to $ N $  \code{Slice}s will scale successfully to thousands of \code{Slice}s. 
We observed the parallel pipelines of DC2 to operate in a stable fashion with a single \code{Pipeline} 
managing on  the order of $\sim40$ \code{Slice}s, but it is not known if this will scale to large numbers. 
Further stress testing of the current approach to larger scales and the investigation of more 
versatile and robust communication and \code{Slice} management are important threads of activity for
DC3 preparation. In any case, the \code{dps} Python and C++ codes provide a stable interface
such that, should we provide alternative implementations, the application codes and the
rest of the middleware will be unaffected.

\subsubsection{Stage}

\paragraph{Description and Usage}

In the LSST baseline the generic class \code{Stage} is used to wrap a single module of 
astronomical pipeline processing.  Developers write application stages by extending the 
\code{Stage} class of \code{dps} package  and implementing the \code{Stage} API. 
For DC2, this  consisted of the  methods \code{preprocess()} and \code{postprocess()} for 
serial elements to be executed within Pipelines, and the
\code{process()} method for parallel processing to be executed by Slice workers. 
This API implementation provides the code hooks by which the application code interfaces with 
the framework of the harness. The current  implementation always occurs in Python, and 
so all application stages are either written in Python or have a top layer of Python that 
invokes underlying C++ code using Python bindings generated by SWIG.  

In addition to providing the generic Stage, for DC2 the \code{dps} package also
featured  special subclasses  of \code{Stage} to provide reusable functionality pertinent  to  
multiple pipelines. These included: 
\code{EventStage}, \code{InputStage/OutputStage}, and \code{SymLinkStage}.

\paragraph{Issues and Lessons Learned}

Each of the elements of the Stage API (\code{preprocess()}, \code{process()}, and  \code{postprocess()}) 
served a needed role in enabling application Stage functionality and in facilitating the
integration of application Stages into the harness framework. The use of Python as the layer for
the final integration did not pose a hurdle to the development of the application Stages. 
The DC2 experience is overall  supportive of the  continued use of this construct for bringing 
application code  into the  pipeline  framework provided by \code{dps}. 


The idea of encapsulating generic, pluggable  functionality within modular Stages (as opposed
to encoded functionality directly into the harness)  was very useful in DC2, 
and will be further developed in future work.



\subsubsection{Clipboard and Queue}

\paragraph{Description and Usage}

The pipeline harness constructs application Stages with an InputQueue and an OutputQueue,
such that for $N_s$ total Stages there are $N_s+1$ Queues; the OutputQueue of Stage $i$ is 
identically the InputQueue of Stage $i+1$. 
Application Stages obtain parameters and data they required from a dictionary type 
structure encapsulated in the Clipboard class; an application Stage pulls the Clipboard
from its InputQueue, retrieves and adds values, and posts the Clipboard to its OutputQueue 
for the next Stage. Though the Queue may contain multiple Clipboards, in DC2 a single Clipboard was used to  carry data and information to application Stages.  

The Clipboard served to insulate application
Stages from issues of inter-pipeline communication and the middleware layer events system.
If  a given application Stage required a trigger event prior to execution, the harness framework
blocked for the arrival of the trigger event. Upon receiving the event, the \code{dps} harness places 
the payload onto the Clipboard of the serial Pipeline and all Slices under the key of the 
incoming event topic.   

In addition, an application Stage could expect to find required data
(for example, image data) on the Clipboard.  Such data would arrive on the Clipboard by 
configuring the Pipeline to begin with (or just include) an InputStage. The InputStage makes use 
of appropriate Persistence framework utilities to read data off of disk into memory and place it on the Clipboard under a  standard key for
the application Stage.   In this way application Stages could be  coded  to expect to 
find required parameters and data on the Clipboard as provided by the middleware harness.

The Clipboard mechanism also plays a role in the  current strategy for inter-pipeline communication.
 A pipeline may send an event to a separate running pipeline by executing an 
EventStage. The EventStage is configured through policy to pull \code{DataProperty}'s off of the
Clipboard under a specified key, and issue an event with these objects as payload under a 
specified event topic.   On the receiving end, the  payload is placed on the Clipboard as 
previously described.  Thus, Clipboards are effectively used to carry information from 
one application Stage to another both within and between running pipelines. 

\paragraph{Issues and Lessons Learned}

Memory management with respect to Clipboards required some care in DC2.
Across multiple visits,  a running Pipeline or Slice  could accumulate data in memory on Clipboards
if proper cleanup/garbage collection did not occur.   In DC2, all Clipboard contents were erased at the
end of the processing for each visit, and it was verified  that running pipelines had stable memory usage 
across multiple visits.  This feature also enforced that there was no dependency of the processing
of one visit on previous ones, which has important consequences for reproducibility of results
in targeted or bulk reprocessing. 

\subsubsection{Exceptions, Logging, and Other Features}

\paragraph{Description and Usage}

Exception handling by the harness framework when invoking the Stage API methods
played an important role in DC2.   
Exceptions thrown within underlying application code and not dealt with in
the lower layers were captured by the framework to prevent problems in one Slice from 
affecting the full parallel computation. If one Slice encountered a fatal exception, this fatal error
was appropriately logged (so as to be visible within the events log/portal) and the affected Slice 
was cycled through its remaining stage loop for the given visit within attempting any further 
computation, and without affecting the other Slices.

The harness also supports the capture of a shutdown event for terminating 
pipeline execution and bringing down the MPI environment cleanly.  The
topic of this event may be set in policy.  The effect of the shutdown event 
is to terminate execution at the end of the current Stage loop, even if other trigger 
visit events have been issued and data awaits processing.  An extension of this idea
will be implemented in the future, whereby a ``final data'' event (an event communicating
that  there is no more data or trigger events  forthcoming) may be issued.  In this case
the execution will terminate after all remaining  trigger visit events and their associated 
data are processed.



% - communication of values between Slices/sharing of information on Clipboards


